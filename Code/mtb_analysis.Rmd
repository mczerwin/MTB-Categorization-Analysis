---
title: "MTB Categorization Analysis"
author: "Michael Czerwinski & Justin Schulberg"
date: "12/29/2021"
output:
  rmdformats::readthedown:
    self_contained: TRUE
    toc_depth: 4
    highlight: tango
    code_folding: hide
    lightbox: TRUE
    gallery: TRUE
    toc_float: # a float toc will stick to the sidebar when scrolling
      collapsed: false
      
---

```{r fig.align='center', fig.height=6, fig.width=6, message=FALSE, warning=FALSE, include=FALSE}
# Set our plot and code block specifications for the rest of the document.
knitr::opts_chunk$set(fig.width = 9,
                      fig.height = 6,
                      fig.align = "center",
                      # Set our code specifications for the rest of the document
                      echo = T,
                      warning = F,
                      message = F)

# Turn off scientific notation
options(scipen=999)

# install.packages("pacman")
# Use pacman to load packages. This'll check to see if a package is already installed; if not, it'll install it. If it is installed, it'll run the library() function.
pacman::p_load(
  readxl, # Used for reading in Excel packages
  tidyverse, # Used for easy data manipulation
  here, # Used for navigating project structures
  kableExtra, # Used for RMarkdown formatting
  DataExplorer, # Used for easy EDA
  corrplot, # Used for correlation plotting
  pander # Pretty printing
  )


# Set our ggplot formats so we don't have to re-type the same code over-and-over again.
theme_set(theme_classic())
theme_update(plot.title = element_text(hjust = 0, color = "slateblue4", size = 20),
  plot.subtitle = element_text(hjust = 0, color = "slateblue2", size = 12),
  plot.caption = element_text(color = "dark gray", size = 10, face = "italic"),
  axis.title.x = element_text(size = 14),
  axis.title.y = element_text(size = 14),
  axis.text.y = element_text(size = 10),
  axis.text.x = element_text(size = 10))

```



# Introduction
For this project, our team will determine whether the specifications of mountain bikes (MTB) are enough to differentiate between the different types of mountain bike categories. 

Currently, full suspension mountain bikes come in multiple categories:

- **Cross Country (XC)** | Tend to be the most lightweight, nimble, and designed to put the rider in an efficient pedaling position 
- **Enduro** | Heavier frames, more travel and more downhill oriented geometry 
- **Trail** | The most common category of bikes, considered to be the halfway point between XC and Enduro
- **All Mountain** | A more niche category which some manufacturers claim to be more downhill focused than trail bikes, but not designed for downhill races like Enduro bikes are
- **Downcountry** | A relatively new category between XC and Trail. Similar to the All Mountain category, these bikes aren’t race specific like XC bikes tend to be, but are lighter and faster than trail bikes.

With all of the factors to consider when designing a bike, there are no clear boundaries between these categories. For example, one brand’s Downcountry bike could be what another brand considers a Trail bike. 

**The goal of our project is to determine how many, if any, discrete categories should exist for mountain bikes.** Since most specifications and geometric measurements have one direction when moving across the spectrum of bikes, it’s reasonable to believe that these measurements could be reduced to much fewer dimensions, and perhaps even one continuous principle component rather than discrete categories. Here is a diagram of some of the different types of geometric specifications on mountain bikes:

![Various Dimension Features of a Bike's Geometry](Images/Bike_Diagram.png)

Let's start by taking a look at the data.

```{r read_data}
# Read in sheet 2 of our data
mtb_data <- read_excel(here::here('Data/mtb_stats.xlsx'), 'Sheet1')
mtb_data <- mtb_data %>% 
  # Clean up the label column
  mutate(label = str_replace_all(str_to_lower(label), '[:punct:]', ''))

# Pull out the class labels
labels <- mtb_data %>% 
  select(label)


# Let's view the mtb_data output
# In any kable outputs, display NAs as blanks
opts <- options(knitr.kable.NA = "")

mtb_data %>% 
  head(25) %>%
  # Fix up the headers by replacing the underscores with spaces
  rename_all(funs(str_replace_all(., "_", " "))) %>% 
  # Make everything proper capitalization
  # rename_all(funs(str_to_title)) %>% 
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = F,
                font_size = 12) %>%
  # Make the header row bold and black so it's easier to read
  row_spec(0, bold = T, color = "black") %>% 
  scroll_box(height = "400px", width = "100%")
```



# EDA

```{r demographics_explore}
DataExplorer::plot_bar(mtb_data, 
                       ggtheme = theme_classic(),
                       title = 'Distribution of Categorical Demographic Variables',
                       theme_config = theme(plot.title = element_text(hjust = 0, 
                                                                          color = "slateblue4", 
                                                                          size = 24),
                                                plot.subtitle = element_text(hjust = 0, color = "slateblue2", size = 10),
                                                plot.caption = element_text(color = "dark gray", size = 10, face = "italic"),
                                                axis.title.x = element_text(size = 14),
                                                axis.title.y = element_text(size = 14)),
                       maxcat = 10,
                       ncol = 2)

DataExplorer::plot_density(mtb_data,
                             ggtheme = theme_classic(),
                             title = 'Distribution of Continuous Demographic Variables',
                             geom_density_args = list(fill = 'slateblue'),
                             theme_config = theme(plot.title = element_text(hjust = 0, 
                                                                                color = "slateblue4", 
                                                                                size = 24),
                                                      plot.subtitle = element_text(hjust = 0, color = "slateblue2", size = 10),
                                                      plot.caption = element_text(color = "dark gray", size = 10, face = "italic"),
                                                      axis.title.x = element_text(size = 14),
                                                      axis.title.y = element_text(size = 14)),
                             ncol = 3)
```



## Average bikes by flip-chip setting
```{r setting}
# Split data based on setting vs. no setting
no_setting <- mtb_data %>% 
  filter(is.na(setting))
setting <- mtb_data %>% 
  filter(!is.na(setting))

mean_by_setting <- aggregate(setting[,-3], list( setting$model), FUN=mean) 
mean_by_setting$model <- mean_by_setting$Group.1
mean_by_setting <- mean_by_setting[,-1]

no_setting <- no_setting[,-3]

new_mtb_data <- rbind(mean_by_setting, no_setting)
```


# Variation Amongst Featureset
The first thing we'll do is look to see if any of the features in our dataset are better at explaining the variation amongst the different bikes than other features. That is, it's completely possible that two features are similar and don't have much variation in them, even across some of the different bike categories. To do so, we'll:

1. Look for highly correlated features and flag these for potential removal;  
2. Run Principal Component Analysis (PCA) to see if certain features are better at explaining the variation in our data better than others.

## 1. Correlation
First, let's take a look at our most highly correlated features. We'll use the `corrplot()` function to better order the highly correlated features by the angular order of their eigenvectors.

```{r correlation}
mtb_correlation <- mtb_data %>% 
  # Get rid of price for now
  select(-price) %>% 
  # Select our variables of interest
  select_if(is.numeric) %>% 
  # Remove rows with NAs in them
  # drop_na() %>% 
  # Build our correlation matrix, such that missing values are handled by casewise deletion
  cor(use = 'complete.obs') 

# Convert our results into a tibble for easier manipulation
mtb_correlation_df <- mtb_correlation %>% 
  as_tibble() %>% 
  mutate(variable = colnames(mtb_correlation)) %>% 
  relocate(variable, everything())

# Build our correlation plot, using the angular order of the eigenvectors
corrplot(mtb_correlation,
         diag = F,
         col = COL2('PRGn'),
         tl.col = 'slateblue4',
         type = 'lower',
         method = 'color',
         order = 'AOE',
         title = 'Mountain Bike Feature Correlation'
         )

```

Here we see some obvious correlations, for example:  
- `f_piston` (front brakes) is perfectly correlated with `r_piston` (rear brakes), which makes sense since mountain bikes tend to use the same types/spec of brakes for the front vs. rear tires.  
- `fork_travel` has a correlation above .95 with: `r mtb_correlation_df %>% select(variable, fork_travel) %>% filter((fork_travel > .95) | (fork_travel < -.95)) %>% select(variable) %>% as.character()`. This make sense; for example, `rear_travel` *should* be highly correlated with `fork_travel`. 

In all, here are the most highly correlated variables (i.e. variables which have a correlation above .95 or below -.95):

```{r high_correlation}
mtb_correlation_df %>% 
  pivot_longer(-variable, 
               names_to = 'correlated_variable', 
               values_to = 'correlation') %>% 
  filter(variable != correlated_variable) %>% 
  filter((correlation > .95) | (correlation < -.95)) %>% 
  pander()

```

For now, we'll opt to include everything. But later on, as we analyze the importance of different features, we'll look to remove some of the above variables first.

## 2. PCA
Next, we'll apply PCA to our dataset. In so doing, we'll have to center and scale our data given how different the ranges are for certain measurements. Let's take a look at our 4 principal components which explain the largest proportion of variance in the data:

```{r pca}

# Impute missing values with column mean (not really best practice, but good enough)

for (c in 1:ncol(mtb_data)){
  if (is.numeric(mtb_data[,c])){
    print(colnames(mtb_data)[c])
    mtb_data[is.na(mtb_data[,c]), c] <- mean(mtb_data[,c], na.rm=TRUE)  
  }
}

mtb_no_null <- mtb_data %>% 
                select(-price) %>% 
                select_if(is.numeric) %>% 
                bind_cols(label = mtb_data$label) %>%
                drop_na()


head(mtb_no_null)


mtb_pca <- prcomp(mtb_no_null %>% select(-label),
                  center = TRUE,
                  scale. = TRUE)

# Put our summary results into a dataframe
mtb_pca_df <- tibble(variable = c('Standard Deviation', 'Proportion of Variance', 'Cumulative Proportion')) %>% 
  cbind(summary(mtb_pca)$importance)


head(mtb_pca_df)

mtb_pca_df %>% 
  # Only display the first 5 columns
  select(c(variable:PC4)) %>% 
  pander()


```

We can see that, actually, starting at our $3^{\text{rd}}$ principal component, nearly `r 100*round(mtb_pca_df %>% filter(variable == 'Cumulative Proportion') %>% select(PC3) %>% as.numeric(), 3)`% of the data's variation is properly explained. Let's take a look at our top 2 principal components:

```{r pca_viz}
pacman::p_load(devtools)
install_github("vqv/ggbiplot")
pacman::p_load(ggbiplot)
g <- ggbiplot(mtb_pca,
              obs.scale = 1,
              var.scale = 1,
              groups = mtb_no_null$label,
              ellipse = TRUE,
              circle = TRUE,
              ellipse.prob = .5)
# g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
               legend.position = 'top')
print(g)

```


# Rowdiness scale


# Clustering


